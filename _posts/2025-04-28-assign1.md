---
layout: distill
title: >-
  Distilling Force Field Foundation Models with Energy Hessians
description: "Machine Learning Force Fields (MLFFs) are becoming vital in computational chemistry. However, the growing size of foundation models poses a significant bottleneck for real-time applications. This post explores a recent ICLR 2025 paper that proposes Hessian distillation, a novel method for transferring knowledge from large MLFF foundation models into smaller and faster specialized models."
date: 2025-06-01
future: true
htmlwidgets: true
hidden: false

authors:
  - name: Junyong Kang

toc:
  - name: Introduction
  - name: Overview
  - name: Preliminaries
  - name: Method
  - name: Analysis
  - name: Review

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .box-note, .box-warning, .box-error, .box-important {
    padding: 15px 15px 15px 10px;
    margin: 20px 20px 20px 5px;
    border: 1px solid #eee;
    border-left-width: 5px;
    border-radius: 5px 3px 3px 5px;
  }
  d-article .box-note {
    background-color: #eee;
    border-left-color: #2980b9;
  }
  d-article .box-warning {
    background-color: #fdf5d4;
    border-left-color: #f1c40f;
  }
  d-article .box-error {
    background-color: #f4dddb;
    border-left-color: #c0392b;
  }
  d-article .box-important {
    background-color: #d4f4dd;
    border-left-color: #2bc039;
  }
  html[data-theme='dark'] d-article .box-note {
    background-color: #333333;
    border-left-color: #2980b9;
  }
  html[data-theme='dark'] d-article .box-warning {
    background-color: #3f3f00;
    border-left-color: #f1c40f;
  }
  html[data-theme='dark'] d-article .box-error {
    background-color: #300000;
    border-left-color: #c0392b;
  }
  html[data-theme='dark'] d-article .box-important {
    background-color: #003300;
    border-left-color: #2bc039;
  }
  html[data-theme='dark'] d-article aside {
    color: var(--global-text-color) !important;
  }
  html[data-theme='dark'] d-article blockquote {
    color: var(--global-text-color) !important;
  }
  html[data-theme='dark'] d-article summary {
    color: var(--global-text-color) !important;
  }
  d-article aside * {
    color: var(--global-text-color) !important;
  }
  d-article p {
    text-align: justify;
    text-justify: inter-word;
    -ms-hyphens: auto;
    -moz-hyphens: auto;
    -webkit-hyphens: auto;
    hyphens: auto;
  }
  d-article aside {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
    font-size: 90%;
  }
  d-article aside p:first-child {
      margin-top: 0;
  }
  d-article details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
  }
  d-article summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
    display: list-item;
  }
  d-article details[open] {
    padding: .5em;
  }
  d-article figure {
    padding: 1em 1em 0;
  }
  d-article details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
  }
  html[data-theme='dark'] d-article blockquote {
    border-left-color: #f1c40f;
  }
  @media (min-width: 768px) {
  .l-gutter:has(> aside) {
      height: 0px;
    }
  }
  @media (min-width: 1025px) {
    d-article d-contents {
      height: 0px;
    }
  }

---

## Introduction

Large foundation models (FMs) for Machine Learning Force Fields (MLFFs), which are trained on large quantities of raw data (ab-initio data), emerging as an alternative computation methods for quantum chemical calculations.
However, despite their generality and accuracy, these models remain **too slow** and **too large** for practical applications like long molecular dynamics (MD) simulations or system-specific optimizations.
In the paper *"Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians"* (ICLR 2025), Amin et al. propose a **Hessian-based knowledge distillation framework** that creates **fast, task-specific MLFFs** while retaining or even surpassing the accuracy of their foundation model teachers.
Notably, this approach is agnostic to model architecture and inductive biases, and the student models achieve inference speeds up to 20 times faster than the original FMs.

---

## Overview: What Is Hessian Distillation?

The core idea is to utilize the knowledge distillation (KD) technique, but rather than using the logit or the feature, the authors employ the second-order derivative matching.
The student model minimizes standard energy and force regression losses as usual, and additionally, we align the **energy Hessian** between the student (derivative of the force field) and teacher (second-order derivative of the energy)

To make training efficient, the authors precompute teacher Hessians.
They also subsample only a few rows of the Hessian during each iteration, and compute student Hessian rows on-the-fly using vector-Jacobian products (VJPs).
Moreover, they propose an optional gradient supervision term to encourage conservative behavior during training without requiring energy gradients at inference.

---

## Preliminaries

Let $$ U_\theta(z, r) $$ be the predicted energy of a molecular configuration defined by atomic numbers $$ z \in \mathbb{R}^n $$ and positions $$ r \in \mathbb{R}^{n \times 3} $$.

Forces: $$
 F_\theta = - \nabla_r U_\theta \in \mathbb{R}^{n \times 3} 
$$

Training loss for MLFFs:
$$
\mathcal{L}_{\text{EF}} = \lambda_U |U_{\text{ref}} - U_\theta|^2 + \lambda_F \sum_{i=1}^n \|F^{(i)}_{\text{ref}} - F^{(i)}_\theta\|^2.
$$

Hessian is the second derivative of the energy w.r.t. atomic positions:
$$
H = \frac{\partial^2 U}{\partial r^2} = -\frac{\partial F}{\partial r} \in \mathbb{R}^{3n \times 3n}.
$$

## Method

### Distillation Loss
The student model $$ S_\phi $$ is trained on a subset $$ \mathcal{D}_{\text{KD}} $$ of the teacher’s data, with supervision from the teacher’s Hessians:

$$
\mathcal{L}(\phi) = \mathcal{L}_{\text{EF}}(\phi) + \lambda_{\text{KD}} \| H_{\text{teacher}} + \frac{\partial F_\phi}{\partial r} \|^2.
$$

### Row Subsampling
Full Hessian computation is costly. Instead, rows are subsampled:

$$
\mathcal{L}(\phi) = \mathcal{L}_{\text{EF}}(\phi) + \lambda_{\text{KD}} \cdot \mathbb{E}_{J \sim U_s(1, 3n)} \left[ \frac{1}{s} \sum_{j \in J} \| H^{(j)} + \partial_j F_\phi \|^2 \right],
$$

which reduces training cost from $$ O(N^2) $$ to $$ O(s) $$, where $$ s \ll 3n $$.

To avoid the full hessian computation, authors use vector-Jacobian products (VJPs).

### Energy Gradient Supervision
To improve the energy predictions of MLFFs with a direct force parameterization, the authors also propose the following loss:

$$
\mathcal{L}_{\nabla U} = \|F + \nabla_r U_\theta\|^2.
$$



## Results

The method was tested on several foundation models and datasets:
- MACE-OFF & EScAIP on **SPICE**
- MACE-MP-0 on **MPtrj**
- JMP (small and large) on **MD22**

As a result, the student models are **up to 20× faster** than FMs, with **up to 50× throughput speedup** when using large batch sizes.
Moreover, Hessian-distilled student models often **outperform undistilled models, traditional KD baselines** on downstream subsets.
They also outpeform the teacher FM, as the student is task-specific.
In particular, improvement is large when the student uses invariant or equivariant architectures.

---

## Analysis

### Student Model Size Ablation
Hessian distillation significantly improves the speed-accuracy tradeoff at all model sizes.
Without distillation, performance plateaus at ~0.28M parameters.
With distillation, performance continues to improve up to 2.3M parameters.
Hence, Hessian supervision acts as a regularizer and allows larger students to scale better.

### Hessian Row Subsampling Ablation
Training time increases roughly linearly with $$s$$. Reducing $$s$$ to as low as 1 has negligible impact on performance, but using just one row per sample maintains accuracy while significantly reducing training cost.

### Force vs. Hessian Supervision
The authors find that distilling Hessians is better than distilling forces, because Hessians encode richer physical information than the first-order gradients.

---

## Review
### Pros
- The idea of Hessian distillation is novel and very effective for MLFFs, in terms of both speed and performance.
- Experiments are solid and practical, as they uses real-world FMs and benchmarks. The ablation study and comparison with the baselines are also solid.

### Cons
- To prepare the dataset, we have to access large teacher FMs. For very large teacher models, computing and storing Hessian information (even with subsampling) might still be costly.
- A deeper theoretical motivation or justification for why Hessian alignment works better than first-order alignment or traditional KD methods is absent. Moreover, the potential inaccuracy of the teacher Hessian should be evaluated, while the authors provide a one case that shows the distilling Hessian leads to better geometric understanding.