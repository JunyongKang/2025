<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},o=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),i=o[0][0],a=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"June 1, 2025"),l="AI810 Blog Post (20257001) - Distilling Force Field Foundation Models with Energy Hessians",r="Machine Learning Force Fields (MLFFs) are becoming vital in computational chemistry. However, the growing size of foundation models poses a significant bottleneck for real-time applications. This post explores a recent ICLR 2025 paper that proposes Hessian distillation, a novel method for transferring knowledge from large MLFF foundation models into smaller and faster specialized models.";{let e=o.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+l.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${l}},\n  abstract = {${r}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${a}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=o.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${l}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>AI810 Blog Post (20257001) - Distilling Force Field Foundation Models with Energy Hessians | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Machine Learning Force Fields (MLFFs) are becoming vital in computational chemistry. However, the growing size of foundation models poses a significant bottleneck for real-time applications. This post explores a recent ICLR 2025 paper that proposes Hessian distillation, a novel method for transferring knowledge from large MLFF foundation models into smaller and faster specialized models."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://junyongkang.github.io.github.io/2025/blog/assign1/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}.box-note,.box-warning,.box-error,.box-important{padding:15px 15px 15px 10px;margin:20px 20px 20px 5px;border:1px solid #eee;border-left-width:5px;border-radius:5px 3px 3px 5px}d-article .box-note{background-color:#eee;border-left-color:#2980b9}d-article .box-warning{background-color:#fdf5d4;border-left-color:#f1c40f}d-article .box-error{background-color:#f4dddb;border-left-color:#c0392b}d-article .box-important{background-color:#d4f4dd;border-left-color:#2bc039}html[data-theme='dark'] d-article .box-note{background-color:#333;border-left-color:#2980b9}html[data-theme='dark'] d-article .box-warning{background-color:#3f3f00;border-left-color:#f1c40f}html[data-theme='dark'] d-article .box-error{background-color:#300000;border-left-color:#c0392b}html[data-theme='dark'] d-article .box-important{background-color:#030;border-left-color:#2bc039}html[data-theme='dark'] d-article aside{color:var(--global-text-color)!important}html[data-theme='dark'] d-article blockquote{color:var(--global-text-color)!important}html[data-theme='dark'] d-article summary{color:var(--global-text-color)!important}d-article aside *{color:var(--global-text-color)!important}d-article p{text-align:justify;text-justify:inter-word;-ms-hyphens:auto;-moz-hyphens:auto;-webkit-hyphens:auto;hyphens:auto}d-article aside{border:1px solid #aaa;border-radius:4px;padding:.5em .5em 0;font-size:90%}d-article aside p:first-child{margin-top:0}d-article details{border:1px solid #aaa;border-radius:4px;padding:.5em .5em 0}d-article summary{font-weight:bold;margin:-.5em -.5em 0;padding:.5em;display:list-item}d-article details[open]{padding:.5em}d-article figure{padding:1em 1em 0}d-article details[open] summary{border-bottom:1px solid #aaa;margin-bottom:.5em}html[data-theme='dark'] d-article blockquote{border-left-color:#f1c40f}@media(min-width:768px){.l-gutter:has(>aside){height:0}}@media(min-width:1025px){d-article d-contents{height:0}}</style> <d-front-matter> <script async type="text/json">{
      "title": "AI810 Blog Post (20257001) - Distilling Force Field Foundation Models with Energy Hessians",
      "description": "Machine Learning Force Fields (MLFFs) are becoming vital in computational chemistry. However, the growing size of foundation models poses a significant bottleneck for real-time applications. This post explores a recent ICLR 2025 paper that proposes Hessian distillation, a novel method for transferring knowledge from large MLFF foundation models into smaller and faster specialized models.",
      "published": "June 1, 2025",
      "authors": [
        {
          "author": "Junyong Kang",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener noopener noreferrer" target="_blank"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener noopener noreferrer" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener noopener noreferrer" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>AI810 Blog Post (20257001) - Distilling Force Field Foundation Models with Energy Hessians</h1> <p>Machine Learning Force Fields (MLFFs) are becoming vital in computational chemistry. However, the growing size of foundation models poses a significant bottleneck for real-time applications. This post explores a recent ICLR 2025 paper that proposes Hessian distillation, a novel method for transferring knowledge from large MLFF foundation models into smaller and faster specialized models.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#overview">Overview</a></div> <div><a href="#preliminaries">Preliminaries</a></div> <div><a href="#method">Method</a></div> <div><a href="#analysis">Analysis</a></div> <div><a href="#review">Review</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Large foundation models (FMs) for Machine Learning Force Fields (MLFFs), which are trained on large quantities of raw data (ab-initio data), emerging as an alternative computation methods for quantum chemical calculations. However, despite their generality and accuracy, these models remain <strong>too slow</strong> and <strong>too large</strong> for practical applications like long molecular dynamics (MD) simulations or system-specific optimizations. In the paper <em>“Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians”</em> (ICLR 2025), Amin et al. propose a <strong>Hessian-based knowledge distillation framework</strong> that creates <strong>fast, task-specific MLFFs</strong> while retaining or even surpassing the accuracy of their foundation model teachers. Notably, this approach is agnostic to model architecture and inductive biases, and the student models achieve inference speeds up to 20 times faster than the original FMs.</p> <hr> <h2 id="overview-what-is-hessian-distillation">Overview: What Is Hessian Distillation?</h2> <p>The core idea is to utilize the knowledge distillation (KD) technique, but rather than using the logit or the feature, the authors employ the second-order derivative matching. The student model minimizes standard energy and force regression losses as usual, and additionally, we align the <strong>energy Hessian</strong> between the student (derivative of the force field) and teacher (second-order derivative of the energy)</p> <p>To make training efficient, the authors precompute teacher Hessians. They also subsample only a few rows of the Hessian during each iteration, and compute student Hessian rows on-the-fly using vector-Jacobian products (VJPs). Moreover, they propose an optional gradient supervision term to encourage conservative behavior during training without requiring energy gradients at inference.</p> <hr> <h2 id="preliminaries">Preliminaries</h2> <p>Let \(U_\theta(z, r)\) be the predicted energy of a molecular configuration defined by atomic numbers \(z \in \mathbb{R}^n\) and positions \(r \in \mathbb{R}^{n \times 3}\).</p> <p>Forces: \(F_\theta = - \nabla_r U_\theta \in \mathbb{R}^{n \times 3}\)</p> <p>Training loss for MLFFs: \(\mathcal{L}_{\text{EF}} = \lambda_U |U_{\text{ref}} - U_\theta|^2 + \lambda_F \sum_{i=1}^n \|F^{(i)}_{\text{ref}} - F^{(i)}_\theta\|^2.\)</p> <p>Hessian is the second derivative of the energy w.r.t. atomic positions: \(H = \frac{\partial^2 U}{\partial r^2} = -\frac{\partial F}{\partial r} \in \mathbb{R}^{3n \times 3n}.\)</p> <h2 id="method">Method</h2> <h3 id="distillation-loss">Distillation Loss</h3> <p>The student model \(S_\phi\) is trained on a subset \(\mathcal{D}_{\text{KD}}\) of the teacher’s data, with supervision from the teacher’s Hessians:</p> \[\mathcal{L}(\phi) = \mathcal{L}_{\text{EF}}(\phi) + \lambda_{\text{KD}} \| H_{\text{teacher}} + \frac{\partial F_\phi}{\partial r} \|^2.\] <h3 id="row-subsampling">Row Subsampling</h3> <p>Full Hessian computation is costly. Instead, rows are subsampled:</p> \[\mathcal{L}(\phi) = \mathcal{L}_{\text{EF}}(\phi) + \lambda_{\text{KD}} \cdot \mathbb{E}_{J \sim U_s(1, 3n)} \left[ \frac{1}{s} \sum_{j \in J} \| H^{(j)} + \partial_j F_\phi \|^2 \right],\] <p>which reduces training cost from \(O(N^2)\) to \(O(s)\), where \(s \ll 3n\).</p> <p>To avoid the full hessian computation, authors use vector-Jacobian products (VJPs).</p> <h3 id="energy-gradient-supervision">Energy Gradient Supervision</h3> <p>To improve the energy predictions of MLFFs with a direct force parameterization, the authors also propose the following loss:</p> \[\mathcal{L}_{\nabla U} = \|F + \nabla_r U_\theta\|^2.\] <h2 id="results">Results</h2> <p>The method was tested on several foundation models and datasets:</p> <ul> <li>MACE-OFF &amp; EScAIP on <strong>SPICE</strong> </li> <li>MACE-MP-0 on <strong>MPtrj</strong> </li> <li>JMP (small and large) on <strong>MD22</strong> </li> </ul> <p>As a result, the student models are <strong>up to 20× faster</strong> than FMs, with <strong>up to 50× throughput speedup</strong> when using large batch sizes. Moreover, Hessian-distilled student models often <strong>outperform undistilled models, traditional KD baselines</strong> on downstream subsets. They also outpeform the teacher FM, as the student is task-specific. In particular, improvement is large when the student uses invariant or equivariant architectures.</p> <hr> <h2 id="analysis">Analysis</h2> <h3 id="student-model-size-ablation">Student Model Size Ablation</h3> <p>Hessian distillation significantly improves the speed-accuracy tradeoff at all model sizes. Without distillation, performance plateaus at ~0.28M parameters. With distillation, performance continues to improve up to 2.3M parameters. Hence, Hessian supervision acts as a regularizer and allows larger students to scale better.</p> <h3 id="hessian-row-subsampling-ablation">Hessian Row Subsampling Ablation</h3> <p>Training time increases roughly linearly with \(s\). Reducing \(s\) to as low as 1 has negligible impact on performance, but using just one row per sample maintains accuracy while significantly reducing training cost.</p> <h3 id="force-vs-hessian-supervision">Force vs. Hessian Supervision</h3> <p>The authors find that distilling Hessians is better than distilling forces, because Hessians encode richer physical information than the first-order gradients.</p> <hr> <h2 id="review">Review</h2> <h3 id="pros">Pros</h3> <ul> <li>The idea of Hessian distillation is novel and very effective for MLFFs, in terms of both speed and performance.</li> <li>Experiments are solid and practical, as they uses real-world FMs and benchmarks. The ablation study and comparison with the baselines are also solid.</li> </ul> <h3 id="cons">Cons</h3> <ul> <li>To prepare the dataset, we have to access large teacher FMs. For very large teacher models, computing and storing Hessian information (even with subsampling) might still be costly.</li> <li>A deeper theoretical motivation or justification for why Hessian alignment works better than first-order alignment or traditional KD methods is absent. Moreover, the potential inaccuracy of the teacher Hessian should be evaluated, while the authors provide a one case that shows the distilling Hessian leads to better geometric understanding.</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>