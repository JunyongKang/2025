<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://junyongkang.github.io.github.io/2025/feed.xml" rel="self" type="application/atom+xml"/><link href="https://junyongkang.github.io.github.io/2025/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-01T17:22:19+08:00</updated><id>https://junyongkang.github.io.github.io/2025/feed.xml</id><title type="html">ICLR Blogposts 2025</title><subtitle>Home to the 2025 ICLR Blogposts track </subtitle><entry><title type="html">AI810 Blog Post (20257001) - Distilling Force Field Foundation Models with Energy Hessians</title><link href="https://junyongkang.github.io.github.io/2025/blog/assign1/" rel="alternate" type="text/html" title="AI810 Blog Post (20257001) - Distilling Force Field Foundation Models with Energy Hessians"/><published>2025-06-01T00:00:00+08:00</published><updated>2025-06-01T00:00:00+08:00</updated><id>https://junyongkang.github.io.github.io/2025/blog/assign1</id><content type="html" xml:base="https://junyongkang.github.io.github.io/2025/blog/assign1/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Large foundation models (FMs) for Machine Learning Force Fields (MLFFs), which are trained on large quantities of raw data (ab-initio data), emerging as an alternative computation methods for quantum chemical calculations. However, despite their generality and accuracy, these models remain <strong>too slow</strong> and <strong>too large</strong> for practical applications like long molecular dynamics (MD) simulations or system-specific optimizations. In the paper <em>“Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians”</em> (ICLR 2025), Amin et al. propose a <strong>Hessian-based knowledge distillation framework</strong> that creates <strong>fast, task-specific MLFFs</strong> while retaining or even surpassing the accuracy of their foundation model teachers. Notably, this approach is agnostic to model architecture and inductive biases, and the student models achieve inference speeds up to 20 times faster than the original FMs.</p> <hr/> <h2 id="overview-what-is-hessian-distillation">Overview: What Is Hessian Distillation?</h2> <p>The core idea is to utilize the knowledge distillation (KD) technique, but rather than using the logit or the feature, the authors employ the second-order derivative matching. The student model minimizes standard energy and force regression losses as usual, and additionally, we align the <strong>energy Hessian</strong> between the student (derivative of the force field) and teacher (second-order derivative of the energy)</p> <p>To make training efficient, the authors precompute teacher Hessians. They also subsample only a few rows of the Hessian during each iteration, and compute student Hessian rows on-the-fly using vector-Jacobian products (VJPs). Moreover, they propose an optional gradient supervision term to encourage conservative behavior during training without requiring energy gradients at inference.</p> <hr/> <h2 id="preliminaries">Preliminaries</h2> <p>Let \(U_\theta(z, r)\) be the predicted energy of a molecular configuration defined by atomic numbers \(z \in \mathbb{R}^n\) and positions \(r \in \mathbb{R}^{n \times 3}\).</p> <p>Forces: \(F_\theta = - \nabla_r U_\theta \in \mathbb{R}^{n \times 3}\)</p> <p>Training loss for MLFFs: \(\mathcal{L}_{\text{EF}} = \lambda_U |U_{\text{ref}} - U_\theta|^2 + \lambda_F \sum_{i=1}^n \|F^{(i)}_{\text{ref}} - F^{(i)}_\theta\|^2.\)</p> <p>Hessian is the second derivative of the energy w.r.t. atomic positions: \(H = \frac{\partial^2 U}{\partial r^2} = -\frac{\partial F}{\partial r} \in \mathbb{R}^{3n \times 3n}.\)</p> <h2 id="method">Method</h2> <h3 id="distillation-loss">Distillation Loss</h3> <p>The student model \(S_\phi\) is trained on a subset \(\mathcal{D}_{\text{KD}}\) of the teacher’s data, with supervision from the teacher’s Hessians:</p> \[\mathcal{L}(\phi) = \mathcal{L}_{\text{EF}}(\phi) + \lambda_{\text{KD}} \| H_{\text{teacher}} + \frac{\partial F_\phi}{\partial r} \|^2.\] <h3 id="row-subsampling">Row Subsampling</h3> <p>Full Hessian computation is costly. Instead, rows are subsampled:</p> \[\mathcal{L}(\phi) = \mathcal{L}_{\text{EF}}(\phi) + \lambda_{\text{KD}} \cdot \mathbb{E}_{J \sim U_s(1, 3n)} \left[ \frac{1}{s} \sum_{j \in J} \| H^{(j)} + \partial_j F_\phi \|^2 \right],\] <p>which reduces training cost from \(O(N^2)\) to \(O(s)\), where \(s \ll 3n\).</p> <p>To avoid the full hessian computation, authors use vector-Jacobian products (VJPs).</p> <h3 id="energy-gradient-supervision">Energy Gradient Supervision</h3> <p>To improve the energy predictions of MLFFs with a direct force parameterization, the authors also propose the following loss:</p> \[\mathcal{L}_{\nabla U} = \|F + \nabla_r U_\theta\|^2.\] <h2 id="results">Results</h2> <p>The method was tested on several foundation models and datasets:</p> <ul> <li>MACE-OFF &amp; EScAIP on <strong>SPICE</strong></li> <li>MACE-MP-0 on <strong>MPtrj</strong></li> <li>JMP (small and large) on <strong>MD22</strong></li> </ul> <p>As a result, the student models are <strong>up to 20× faster</strong> than FMs, with <strong>up to 50× throughput speedup</strong> when using large batch sizes. Moreover, Hessian-distilled student models often <strong>outperform undistilled models, traditional KD baselines</strong> on downstream subsets. They also outpeform the teacher FM, as the student is task-specific. In particular, improvement is large when the student uses invariant or equivariant architectures.</p> <hr/> <h2 id="analysis">Analysis</h2> <h3 id="student-model-size-ablation">Student Model Size Ablation</h3> <p>Hessian distillation significantly improves the speed-accuracy tradeoff at all model sizes. Without distillation, performance plateaus at ~0.28M parameters. With distillation, performance continues to improve up to 2.3M parameters. Hence, Hessian supervision acts as a regularizer and allows larger students to scale better.</p> <h3 id="hessian-row-subsampling-ablation">Hessian Row Subsampling Ablation</h3> <p>Training time increases roughly linearly with \(s\). Reducing \(s\) to as low as 1 has negligible impact on performance, but using just one row per sample maintains accuracy while significantly reducing training cost.</p> <h3 id="force-vs-hessian-supervision">Force vs. Hessian Supervision</h3> <p>The authors find that distilling Hessians is better than distilling forces, because Hessians encode richer physical information than the first-order gradients.</p> <hr/> <h2 id="review">Review</h2> <h3 id="pros">Pros</h3> <ul> <li>The idea of Hessian distillation is novel and very effective for MLFFs, in terms of both speed and performance.</li> <li>Experiments are solid and practical, as they uses real-world FMs and benchmarks. The ablation study and comparison with the baselines are also solid.</li> </ul> <h3 id="cons">Cons</h3> <ul> <li>To prepare the dataset, we have to access large teacher FMs. For very large teacher models, computing and storing Hessian information (even with subsampling) might still be costly.</li> <li>A deeper theoretical motivation or justification for why Hessian alignment works better than first-order alignment or traditional KD methods is absent. Moreover, the potential inaccuracy of the teacher Hessian should be evaluated, while the authors provide a one case that shows the distilling Hessian leads to better geometric understanding.</li> </ul>]]></content><author><name>Junyong Kang</name></author><summary type="html"><![CDATA[Machine Learning Force Fields (MLFFs) are becoming vital in computational chemistry. However, the growing size of foundation models poses a significant bottleneck for real-time applications. This post explores a recent ICLR 2025 paper that proposes Hessian distillation, a novel method for transferring knowledge from large MLFF foundation models into smaller and faster specialized models.]]></summary></entry><entry><title type="html">AI810 Blog Post (20257001) - ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design</title><link href="https://junyongkang.github.io.github.io/2025/blog/assign2/" rel="alternate" type="text/html" title="AI810 Blog Post (20257001) - ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design"/><published>2025-06-01T00:00:00+08:00</published><updated>2025-06-01T00:00:00+08:00</updated><id>https://junyongkang.github.io.github.io/2025/blog/assign2</id><content type="html" xml:base="https://junyongkang.github.io.github.io/2025/blog/assign2/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Ligand-based drug design aims to develop small molecules that mimic the 3D interaction profiles (shape, electrostatics, pharmacophores) of known actives. Traditionally, this involves similarity-based virtual screening using shape or electrostatic scores. However, this method is limited to searching existing libraries, often fails on flexible molecules, and is computationally expensive.</p> <p>ShEPhERD (Shape, Electrostatics, and Pharmacophore Explicit Representation Diffusion) addresses this by introducing a generative model that learns to jointly generate 3D molecules and their interaction profiles from noise. This allows to conditional generation of molecules with injecting specific chemical information into the generation process (inpainting).</p> <hr/> <h1 id="method">Method</h1> <p>ShEPhERD introduces a generative model to sample chemically diverse molecules as 3D molecular graph, conditioned on interaction profiles defined by molecular <strong>shape</strong>, <strong>electrostatics</strong>, and <strong>pharmacophores</strong> (features of a molecule that are required for it to bind to a biological target). The model learns a <strong>joint denoising diffusion probabilistic model (DDPM)</strong> over molecular graphs and their interaction profiles. We briefly summarize (1) the representations of variables, (2) similarity scoring functions, (3) the joint diffusion process and (4) the denoising network architecture.</p> <h2 id="1-molecular-and-interaction-representations">1. Molecular and Interaction Representations</h2> <h3 id="11-3d-molecular-graph-x_1">1.1 3D Molecular Graph (\(x_1\))</h3> <p>The base molecular representaion is defined as a 3D graph.</p> <p>Each molecule is defined as:</p> <ul> <li>\(a \in \{H, C, N, O, ...\}^{n_1}\): atom types, one-hot encoded</li> <li>\(f \in \{-2, -1, 0, 1, 2\}^{n_1}\): formal charges</li> <li>\(C \in \mathbb{R}^{n_1 \times 3}\): atomic coordinates</li> <li>\(B \in \{0, 1, 2, 3, 1.5\}^{n_1 \times n_1}\): bond orders (includes aromatic bonds)</li> </ul> <p>Note that categorical features are relaxed to continuous embeddings for diffusion model.</p> <h3 id="12-shape-x_2">1.2 Shape (\(x_2\))</h3> <p>The molecular <strong>shape</strong> is represented as a <strong>surface point cloud</strong> \(S_2 \in \mathbb{R}^{n_2 \times 3}\), sampled from the <strong>solvent-accessible surface (SAS)</strong> of the molecule.</p> <h3 id="13-electrostatics-x_3">1.3 Electrostatics (\(x_3\))</h3> <p>Electrostatic potential (ESP) is encoded as:</p> <ul> <li>\(S_3 \in \mathbb{R}^{n_3 \times 3}\): surface points (same as shape sampling)</li> <li>\(v \in \mathbb{R}^{n_3}\): ESP values computed from the per-atom partial charges of \(x_1\)</li> </ul> <p>Each point’s potential is computed via Coulombic summation (see Appendix).</p> <h3 id="14-pharmacophores-x_4">1.4 Pharmacophores (\(x_4\))</h3> <p>Pharmacophores are represented as:</p> <ul> <li>\(p \in \mathbb{R}^{n_4 \times N_p}\): one-hot pharmacophore types (\(N_p\) types)</li> <li>\(P \in \mathbb{R}^{n_4 \times 3}\): pharmacophores’ coordinates</li> <li>\(V \in \{\mathbb{S}^2, \mathbf{0}\}^{n_4}\): directional unit vectors (for directional types of pharmacophores only)</li> </ul> <p>Directional types include hydrogen bond donors/acceptors, aromatic rings, and halogen bonds.</p> <hr/> <h2 id="2-3d-similarity-scoring-functions">2. 3D Similarity Scoring Functions</h2> <p>ShEPhERD develop own 3D similarity scoring functions for shape, ESP, and Pharmacophore. These functions natively operate on the molecular representations in this paper. Similar to the traditional scoring functions, they develop Gaussian-overlap based Tanimoto similarity scores across different domains.</p> <h3 id="21-general-form">2.1 General Form</h3> <p>The authors define Tanimoto similarity from the overlap of two point clouds: \(\text{sim}^*(Q_A, Q_B) = \frac{O_{A,B}}{O_{A,A} + O_{B,B} - O_{A,B}}, \quad O_{A,B} = \sum_{a \in Q_A} \sum_{b \in Q_B} w_{ab} \left(\frac{\pi}{2\alpha}\right)^{3/2} e^{-\frac{\alpha}{2}\|\mathbf{r}_a - \mathbf{r}_b\|^2},\)</p> <p>where each point \(\mathbf{r}_k\) from a point cloud \(Q\) is assigned an isotropic Gaussian. As these similarities are sensitive to \(SE(3)\) transformations, the final similarity is computed as the maximum of \(\text{sim}^*\) under the transformation.</p> <h3 id="22-shape-similarity">2.2 Shape Similarity</h3> <ul> <li>Uses either <strong>volumetric</strong> atom centers or <strong>surface</strong> point clouds</li> <li>Calibrated to volumetric baseline with tuned \(\alpha\) values</li> </ul> <h3 id="23-esp-similarity">2.3 ESP Similarity</h3> <p>Adds exponential term on ESP values: \(w_{ab} = \exp\left(-\frac{(v_a - v_b)^2}{\lambda}\right)\)</p> <h3 id="24-pharmacophore-similarity">2.4 Pharmacophore Similarity</h3> <p>Type-aggregated cosine-weighted similarity: \(w_{ab;m} = \begin{cases} 1 &amp; \text{non-directional} \\ \frac{\vec{V}_a^\top \vec{V}_b + 2}{3} &amp; \text{directional} \end{cases}\)</p> <hr/> <h2 id="3-diffusion-process-and-denoising-network-architecture">3. Diffusion Process and Denoising Network Architecture</h2> <h3 id="31-diffusion-process-and-denoising-model">3.1 Diffusion Process and Denoising model</h3> <p>A joint diffusion model over \(X = (x_1, x_2, x_3, x_4)\) is trained, following the DDPM formulation.</p> <p>Forward noising process is defined by \(x^{(t)} = \alpha_t x^{(t-1)} + \sigma_t \epsilon, \quad \epsilon \sim \mathcal{N}(0, I),\) where \(t\) denotes the (discretized) diffusion timestep.</p> <p>and coordinate and directional features are handled with centering and normalization. The reverse process involves the denoising model output \(\hat{\epsilon}^{(t)}\):</p> \[x^{(t-1)} = \frac{1}{\alpha_t} \left(x^{(t)} - \frac{\sigma_t^2}{\sigma_t \alpha_t} \hat{\epsilon}^{(t)}\right) + \sigma_t \frac{\sigma_{t-1}}{\sigma_t} \epsilon'.\] <p>To build geometry-aware denoising architecture, the authors uses <strong>EquiformerV2</strong><d-cite key="liao2024equiformerv"></d-cite> built on \(SE(3)\)-equivariant <strong>E3NNs</strong><d-cite key="geiger2022e3nn"></d-cite> with 3 components: embedding modules, joint module, then the denoising module.</p> <h3 id="32-embedding-module">3.2 Embedding Module</h3> <p>Each \((x_i^t,t)\) is encoded into:</p> <ul> <li>Invariant scalar representation: \(z_i^t = \in \mathbb{R}^{n_i \times d}\)</li> <li>Equivariant vector representations: \(\tilde{z}_i^t \in \mathbb{R}^{n_i \times 3 \times d}\) using EquiformerV2.</li> </ul> <h3 id="34-joint-module">3.4 Joint Module</h3> <ul> <li>Local interaction: use another EquiformerV2 module to residually update each nodes’ latent features \((z_i^t, \tilde{z}_i^t)\).</li> <li>Global interaction: Sum-pool the updated vector node features (by local intercation), concatenate them, and then embed with an equivariant feed-forward network. Then apply equivariant tensor products between this vector global feature with the l=0 embedding of \(t\).</li> <li>Add the global interaction output to the local node embeddings.</li> </ul> <h3 id="35-denoising-module">3.5 Denoising Module</h3> <ul> <li>Scalar predictions via MLPs</li> <li>Vector predictions via equivariant feed-forward networks</li> <li>Bonds predicted symmetrically from node pairs The network is trained by noise prediction, and the authors train the model to learn \(P(x_1)\), \(P(x_1, x_2)\), \(P(x_1, x_3)\), \(P(x_1, x_4)\), and \(P(x_1, x_3, x_4)\).</li> </ul> <h3 id="36-sampling">3.6 Sampling</h3> <p>After training the denoising module, we can perform unconditional/conditional generation. Unconditional sampling starts from Gaussian prior, apply DDPM denoising, and then discretize the final values. For the conditional sampling, the authors utilize inpainting. They fix noisy interaction profiles \(x_2, x_3, x_4\) during denoising and only molecule structure \(x_1\) is updated to match the interaction constraints.</p> <hr/> <h1 id="experiments">Experiments</h1> <p>ShEPhERD is trained and evaluated on two custom datasets:</p> <ul> <li><strong>ShEPhERD-GDB17</strong>: 2.8M molecules with ≤17 non-H atoms, optimized in gas phase.</li> <li><strong>ShEPhERD-MOSES-aq</strong>: 1.6M MOSES molecules with ≤27 non-H atoms, optimized in implicit water.</li> </ul> <h2 id="1-unconditional-joint-generation">1. Unconditional Joint Generation</h2> <p><strong>Objective</strong>: Assess ShEPhERD’s ability to jointly generate 3D molecule, by evaluating whether generated interaction profile matches the true interaction profile of the generated molecule.</p> <p><strong>Results</strong>: Comparing the similarities \(\text{sim}_{surf}\), \(\text{sim}_{ESP}\), and \(\text{sim}_{pharmacophore}\) between the true profiles of the generated molecules and those of random molecules from the dataset, ShEPhERD’s generated profiles have enriched similarities to the true profiles in all cases.</p> <h2 id="2-interaction-conditioned-generation">2. Interaction-Conditioned Generation</h2> <p><strong>Objective</strong>: Generate molecules from fixed interaction profiles via inpainting.</p> <h3 id="results">Results:</h3> <ul> <li>While &gt;94% of generated molecules have graph similarity &lt;0.2, these molecules show high 3D similarity over random dataset baselines (Figure 3 shows qualitative examples for directional pharmacophores)</li> </ul> <h2 id="3-natural-product-ligand-hopping">3. Natural Product Ligand Hopping</h2> <p><strong>Objective</strong>: A downstream task to mimic complex natural products with drug-like molecules.</p> <h3 id="setup">Setup:</h3> <ul> <li>3 complex natural products from COCONUT</li> <li>Conditional generation:\(P(x_1, x_3, x_4)\).</li> <li>Baselines: <ul> <li>Virtual screening (2,500 dataset samples)</li> <li>REINVENT (10,000 samples with reward = 3D similarity)</li> </ul> </li> </ul> <h3 id="results-1">Results:</h3> <ul> <li>ShEPhERD generates simpler structures with high 3D similarity</li> <li>Outperforms REINVENT in top-1 similarity, with fewer samples and no optimization</li> </ul> <h2 id="4-bioactive-hit-diversification">4. Bioactive Hit Diversification</h2> <p><strong>Objective</strong>: Diversify chemical structures (PDB ligands) while preserving their interaction profiles.</p> <h3 id="setup-1">Setup:</h3> <ul> <li>Generate 500 samples using \(P(x_1, x_3, x_4)\).</li> <li>Vina docking scores as weak surrogate for bioactivity</li> </ul> <h3 id="results-2">Results:</h3> <ul> <li>ShEPhERD improves top-1 Vina scores in 6/7 cases</li> </ul> <h2 id="5-bioisosteric-fragment-merging">5. Bioisosteric Fragment Merging</h2> <p><strong>Objective</strong>: Merge 13 fragment hits into a single ligand based on their combined interaction profiles.</p> <h3 id="setup-2">Setup:</h3> <ul> <li>Use pharmacophores and ESP from 13 fragments as the condition.</li> <li>Generate 1,000 candidates with \(P(x_1, x_3, x_4)\).</li> </ul> <h3 id="results-3">Results:</h3> <ul> <li>ShEPhERD produces top-10 molecules with good alignment and low SA score <d-cite key="sascore"></d-cite> (simplicity metric)</li> </ul> <h2 id="6-summary">6. Summary</h2> <p>ShEPhERD consistently generates novel, unique molecules with high fidelity to interaction profiles. In ligand-based downstream tasks, even without knowing the protein, generated molecules maintain or improve binding scores.</p> <hr/> <h1 id="review">Review</h1> <h2 id="pros">Pros</h2> <ol> <li>This paper explicitly models not only molecular geometry but also interaction-driving features (ESP, pharmacophores) in a symmetry-aware framework. which is a novel modeling approach for 3D molecular generation.</li> <li>Thanks to the joint modeling, conditional generation performance in several downstream tasks surpasses baseline methods.</li> <li>Technical novelty on modeling representations and networks for various interaction features and designing custom scoring functions.</li> </ol> <h2 id="cons">Cons</h2> <ol> <li>While the paper provides significant improvement in the field of conditional generation of molecules, some experimental results shows low success rate and consistency, for instance, in pharmacophore generation.</li> <li>While useful for ligand-based design, structure-based scenarios still require separate adaptation.</li> <li>There are many technical engineering in the paper which makes readers hard to follow up, while these do not diminish the core methodological advance.</li> </ol>]]></content><author><name>Junyong Kang</name></author><summary type="html"><![CDATA[ShEPhERD introduces a 3D generative diffusion model that jointly generates molecular structures and their chemical interaction profiles. The key feature of this model is that it enables interaction-conditioned generation of molecules through the diffusion denoising process, which allows to solve ligand-based design tasks like natural product mimicking, hit diversification, and fragment merging.]]></summary></entry></feed>